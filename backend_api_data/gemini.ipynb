{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731792992.612643 7016086 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U google-generativeai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tenacity import retry\n",
    "import instructor\n",
    "import google.generativeai as genai\n",
    "from pydantic import BaseModel\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_key = os.getenv('GEMINI_API_KEY')\n",
    "genai.configure(api_key=gemini_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  349k  100  349k    0     0  2212k      0 --:--:-- --:--:-- --:--:-- 2224k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731792996.771594 7016086 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n"
     ]
    }
   ],
   "source": [
    "!curl -o jetpack.jpg https://storage.googleapis.com/generativeai-downloads/images/jetpack.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731793001.630281 7016086 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file 'Jetpack drawing' as: https://generativelanguage.googleapis.com/v1beta/files/devv7xdkdrga\n"
     ]
    }
   ],
   "source": [
    "sample_file = genai.upload_file(path=\"PumpkinPie2-2.jpg\",\n",
    "                            display_name=\"Jetpack drawing\")\n",
    "\n",
    "\n",
    "sample_file_uri = sample_file.uri\n",
    "print(f\"Uploaded file '{sample_file.display_name}' as: {sample_file.uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved file 'Jetpack drawing' as: https://generativelanguage.googleapis.com/v1beta/files/devv7xdkdrga\n"
     ]
    }
   ],
   "source": [
    "file = genai.get_file(name=sample_file.name)\n",
    "print(f\"Retrieved file '{file.display_name}' as: {sample_file.uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       ">Pumpkin pie\n",
       "\n",
       "150g (Estimate for a single slice, the whole pie could be around 1000-1200g)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tenacity import stop_after_attempt\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-pro-latest\")\n",
    "\n",
    "# Prompt the model with text and the previously uploaded image.\n",
    "@retry(stop=stop_after_attempt(7))\n",
    "def generate_image(prompt):\n",
    "    response = model.generate_content([sample_file, prompt])\n",
    "    return response\n",
    "prompt =  \"What is the name of the product in the image, do not give a whole sentence and do not describe the image. Also predict the weight of the item in the image in grams \"\n",
    "response = generate_image(prompt)\n",
    "Markdown(\">\" + response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Description(BaseModel):\n",
    "    name: str\n",
    "    weight: int\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [\n",
    "    \"What is the name of the product in the image, do not give a whole sentence and do not describe the image. Also predict the weight of the item in the image in grams \",\n",
    "    sample_file,  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = instructor.from_gemini(\n",
    "    client=genai.GenerativeModel(\n",
    "        model_name=\"models/gemini-1.5-flash-latest\",\n",
    "    ),\n",
    "    mode=instructor.Mode.GEMINI_JSON,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.create(\n",
    "    response_model=Description,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content,\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
